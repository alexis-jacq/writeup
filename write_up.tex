%\documentclass[10pt,a4paper,twocolumn]{article}
\documentclass[10pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx} 
\usepackage{subfigure}
\usepackage{paralist}
\usepackage{hyperref}
\usepackage{comment} %for anonymized version everything removed for anonymization is in \excludecomment{}

\usepackage{hyperref,xcolor}
%\usepackage{algpseudocode}
\usepackage[lined,boxed]{algorithm2e}
\usepackage[english]{babel}
\usepackage{graphicx}
\usepackage{url}
\usepackage{geometry}
\usepackage{mathrsfs}
\usepackage{color}
\usepackage{sidecap}
\usepackage{bbold}
\usepackage{caption}

\usepackage{ gensymb } % for \degree
\usepackage{ amssymb } % for \varnothing

\definecolor{canard}{rgb}{0,0.4,0.2}
\definecolor{wine}{rgb}{0.4,0,0.2}
\definecolor{marron}{rgb}{0.9,0.4,0.1}
\hypersetup{
  colorlinks,
  linkcolor=canard
}

%\definecolor{canard}{rgb}{1,0,1}
%\definecolor{canard}{rgb}{0,1,1}



\DeclareMathOperator*{\argmin}{\arg\!\min}
\DeclareMathOperator*{\argmax}{\arg\!\max}

\geometry{left=3.5cm , right=3cm}


\usepackage{url}
\usepackage{booktabs}

\usepackage{tikz}
\usetikzlibrary{arrows, decorations.markings}
\usetikzlibrary{fit}
\usetikzlibrary{positioning, calc}

\usepackage[draft,nomargin,footnote]{fixme}

\graphicspath{{figs/}}

\usepackage{xspace}
\newcommand{\eg}{\textit{e.g.}\xspace}
\newcommand{\etal}{\textit{et al.}\xspace}
\newcommand{\ie}{\textit{i.e.}\xspace}
\newcommand{\etc}{\textit{etc.}\xspace}
\newcommand{\vs}{\textit{vs.}\xspace}

\newcommand\given[1][]{\:#1\vert\:}

\begin{document}


\title{Mutual Modelling in Educational Child-Robot Interaction}


%\author{\# \# \#}
\author{Alexis Jacq$^{1,2}$\\
$^1$CHILI Lab, \'Ecole Polytechnique F\'ed\'erale de Lausanne, Switzerland,\\
$^2$Instituto Superior T\'{e}cnico, University of Lisbon, Portugal}



\maketitle
\begin{abstract}
In any constructive social interaction, agents must be able to understand each other. This mutual understanding requires the ability to establish a mental model of the other, called mutual modelling. My \textit{Ph.D.} is focused on mutual modelling in robotics: How robots can model other agents within social activities? Such models must be dynamic in order to keep track of shared knowledge, and adaptive in order to deal with specific behaviours of agents. 
This work is directly applied to the CoWriter Project, that aims at exploring how a robot can help children with the acquisition of handwriting~\footnote{http://chili.epfl.ch/cowriter}. Does a second level of mutual modelling in robots would actually improve educative Human-Robot Interactions?
\end{abstract}
%\newpage

%\tableofcontents

\section{Introduction}
\subsection{Importance of higher levels of Mutual Modelling in HRI}

As human individuals, most of our skills and knowledge have been built through social interactions. At the beginning we learn from parents language and basics of etiquette.  Older, we learn culture and science mainly at school with teachers. To make possible such a social learning, both learners and teachers need to model each other: a teacher has to be aware of understood knowledge of his student, and a learner has to take the perspective of his teacher to understand his lessons. 

Developed by Baron-Cohen and Leslie~\cite{baron1985does}, the Theory of Mind (ToM) describes the ability to attribute mental states and knowledge to others. In interaction, humans are permanently collecting and analysing huge quantity of information to stay aware of emotions, goals and understandings of their fellows. In this work, we focus on a generalization of this notion: Mutual Modelling characterizes the effort of one agent to model the mental state of another one~\cite{dillenbourg1999you}. 

Until now, the work conduced by the Human-Robot Interaction (HRI) community to develop mutual modelling abilities in robots was limited to a first level of modelling (see Related Work section~\ref{rw}). Higher levels require the ability to recursively attribute a theory of mind to other agent (\textit{I think that you think that} ...) and their application to HRI remains unexplored. However, a knowledge of oneself perceived by others is necessary to adapt a behaviour to keep mutual understandings. Without such a reflection it is possible to try sharing knowledge, but no immediate feedbacks of success are given. If the trial has failed it is impossible to guess it and to repair it. % develop !

Especially in a context of child-robot interaction, a robot must take care of his image: how much it is perceived as an automatic and boring agent, or contrariwise as a surprising and intelligent character. If the robot is able to detect this perception of itself, it can adapt its behaviour in order to be understood: ``you think I am sad while I am happy, I want you to understand that I am happy". In an educational context where knowledge is shared, agents must exhibit that they are acquiring the shared information with an immediate behaviour: ``I look what you are showing to me, do you see I am looking what you are showing, do you think I am paying attention to your explanation ?"; ``I have understood your idea, do you understand I have understood ?". We have different strategies to exhibit understanding or to repair a miss-understanding. As an example, if someone is talking about a visual object, we alternatively gaze at the object and at the person to make sure he saw we gazed at the object. Or if we detect that the other have not understood a gesture (e.g. pointing an object) we probably exaggerate the gesture.

The goal of this thesis is to develop a cognitive architecture that enables robots to perform mutual modelling-based reasoning in order to reach such a behaviour. It requires the robot to be aware of itself, of the human, and of itself perceived by the human. 

Our research question is: \textit{does a second level of modelling enables higher quality interactions?} We will operationalize this questions and the measures of quality in this document.

\subsection{Application to the CoWriter project}
Children facing difficulties in handwriting integration are more exposed
to troubles during the acquisition of other disciplines as they grow up
\cite{Christensen2005}. 
The CoWriter activity introduces a new approach to help those children
\cite{Hood}. Based on \emph{learning by teaching} paradigm, it aims to repair self-confidence and motivation of the child rather than his handwriting performance alone.

\emph{Learning by teaching} is a technique that engages students to conduct the activity in the role of the teachers in order to support their learning process. This 
paradigm is known to produce motivational, meta-cognitive and educational
benefits in a range of disciplines~\cite{Rohrbeck2003}. The CoWriter project
is the first application of learning by teaching approach to handwriting. 

The effectiveness of our learning by teaching activity is built on the
``prot\'eg\'e effect'': the teacher feels responsible for his student, commits
to the student's success and possibly experiences student's failure as his own
failure to teach. The main idea is to promote in the child an extrinsic motivation to write letters (he will do it in order to help his ``prot\'eg\'e" robot) and to reinforce the self-esteem of the child (he plays the teacher and the robot actually progress).

Our project requires a robot that play the role of a realistic learner to attract and challenge the child. At the moment, we have already implemented and tested the activity without mutual modelling ability in the robot~\cite{Hood}: the movements, the learning curve and the sentences of the robot are set beforehand and do not depend on the interaction. However, the realism of the learning robot and the prot\'eg\'e effect could be strongly improved if the robot was able to play his role according to a model of the child and, at a deeper level, to a model of \textit{the robot-perceived-by-the-child} and \textit{the child-perceived-by-himself}.

With a mutual modelling architecture, the robot could adapt his learning speed based on these models to make sure the child perceive its progresses. Furthermore, we want a robot that implicitly leads the child to correct himself while correcting the robot: for example, it could detect and exaggerate the mistakes of the child. At another scale, the robot could have a physical behaviour adapted to the child: \textit{micro-behaviours} (gestures, gazes and sounds) must be based on the content of the interaction and on the behaviour of the child in order to improve the realism of the robot, to induce strong prot\'eg\'e effect and to make the interaction with the robot more comfortable for a young child. 

In this thesis, we aim to construct a general framework for mutual modelling and, in the same time, to develop and test it through this activity. We have designed an experimental framework that enable long-term studies in real pedagogic/therapeutic contexts~\cite{jacq2016building}. Following this work, our results will be based on experimental studies with children that face actual difficulties in learning handwriting.

\section{Related work}\label{rw}


In several contexts, a large amount of fields have introduced their own framework to describe mutual modelling ability~\cite{lemaignan2015mutual}. 
In Developmental psychology, Flavell~\cite{flavell1990developmental} denotes two different levels of perspective taking: The \textit{cognitive connection} (I see, I hear, I want, I like...) and \textit{mental representation} (what other agents feel, hear, want...). A specific deficit, namely autistic spectrum disorder, is accompanied by a lake of theory of mind in affected children. The study of this disorder led psychologists to develop false-belief experiments\cite{baron1985does}\cite{frith1994autism} that are used to bring out different levels of ToM in children.

In Psycholinguistics and collaborative learning, and more precisely in \textit{computer supported collaborative learning} (CSCL), Roschelle and Teasley~\cite{roschelle1995construction} suggested that collaborative learning requires a process of constructing and maintaining a \textit{shared understanding} of the task at hand. 
The term ``mutual modelling" was introduced in this context, and focused on knowledge states of agents~\cite{dillenbourg1999you}. Dillenbourg developed in \cite{sangin2007partner} a computational framework to represent mutual modelling situations. He introduced the notation \textbf{M(A,B,X)} to denote ``\textbf{A} knows that \textbf{B} knows \textbf{X}".

Epistemic logic uses another notation to describe this concept: $\textbf{K}_{\textbf{A}}\textbf{K}_{\textbf{B}}\textbf{X}$ where $\textbf{K}_{\textbf{i}}\textbf{X}$ stands for ``agent \textbf{i} knows \textbf{X}. This notation is extended in order to describe more complex situations, like the \textit{shared-knowledge} (all the agents of a group know \textbf{X}) and the \textit{common-knowledge} (all the agents of a group know \textbf{X}, and know that all the agent know \textbf{X}, and know that all the agents know that all the agents know \textbf{X}..., etc.)~\cite{hendricks2008epistemic}. 

However, HRI research has not, until now, explored the whole potential of mutual modelling. In \cite{scassellati2002theory}, Scassellati gave an initial account of Leslie's and Baron-Cohen's theory of mind from the perspective of robotics. He limited his work to perceptual precursors (face detection or colour saliency detection). Since then, some work (including Breazeal~\cite{breazeal2006using}, Trafton~\cite{Trafton2005}, Ros~\cite{Ros2010} and Lemaignan~\cite{lemaignan2012thesis}) has been conduced to implement Flavell's first level of perspective taking~\cite{flavell1977development} (``\textit{I see (you do not see the book)}"), ability that is limited to the process of visual perception. 



\section{Development \& experiments with CoWriter}
During the first year of my \textit{Ph.D.} I developed the CoWriter activity. I improved the content of the interaction (algorithm to learn and generate letters, automatic behaviour of the robot, interface to chose words on the second tablet, buttons for children feedback...) and the reliability and robustness of the implementation in order to make it usable by a child over an hour. Then, I tested this work via two case-studies and one clinic-study in real therapeutic contexts (see \ref{long}). I also contributed to the development of a tool that computes in real-time the visual focus of attention of a child and estimates a value of ``with-me-ness" (see \ref{with}).

\subsection{Interaction overview}
Figure~\ref{experimental_setup} illustrates our general experimental setup: a
face-to-face child-robot interaction with an (autonomous) Aldebaran's {\sc nao}
robot.

A tactile tablet (with a custom application) is used by both the robot and the
child to write: in each turn, the child requests the robot to write
something (a single letter, a number or a full word), and pushes the tablet
towards the robot, the robot writes on the tablet by gesturing the writing (but
without actually physically touching the tablet). The child then pulls back the
tablet, corrects the robot's attempt by writing himself on top or next to
the robot's writing, and ``sends'' his
demonstration to the robot by pressing a small button on the tablet. The robot
learns from this demonstration and tries again.


   \begin{figure}
       \centering
       \includegraphics[width=0.6\columnwidth]{experimental_setup}
       \caption{\small Our experimental setup: face-to-face interaction with a {\sc
           nao} robot.  The robot writes on the tactile tablet, the child then
           corrects the robot by directly overwriting its letters on the tablet
           with a stylus. An adult (either a therapist or an experimenter,
           depending on the studies), remains next to the child to guide the work
           (prompting, turn taking, etc.). For some studies, a second tablet and an
           additional camera (lightened) are employed.}

       \label{experimental_setup}
   \end{figure}
   
In \cite{jacq2016building}, we added buttons to the tablet interface to allow the child to evaluate the robot. Even if this feedback does not necessarily concerns the progress of the robot's writing (children also use them to express how they like the robot), we can still exploit and improve this idea to get effective information about the child's perception of the robot.
   
The robotic implementation of this activity is explained in \cite{Hood}. We used ROS to ensure the synchronization and communication between different devices. Likewise, the cognitive architecture for mutual modelling will be deigned as a ROS module that will collect information about the mental state of agents (here, the child, or \textit{the robot-perceived-by-the-child}, or \textit{the child perceived by himself}) in order to update its model of these agents. Then, other modules (for example the module that governs the learning algorithm of the robot) will use these models to make decisions. We will better explain this design below, in section [cognitive architecture / implementation].

\subsection{Generating and learning letters}

Since our approach is based on teaching a robot to write, generating (initially
bad) letters and learning from demonstrations is a core aspect of the project.
The initial state of the robot and his ability to obviously learn from demonstrations of the child is the key to lend credibility to the activity and to induce the ``prot\'eg\'e" effect.

The technical idea is simple: allographs of letters are encoded as a sequence of 70 points in
2D-space and can be seen as vectors with 140 elements
($x_1,...,x_{70},\hspace{1mm}y_1,...,y_{70}$). We arbitrary chose a set of allograph
that define the initial state of generated letters. 
After the child provided a demonstration of a letter, the algorithm
generates a new letter corresponding to the middle point between the last state and the
demonstration. Details of this algorithm are presented in \cite{Hood} and improvements are tested in \cite{jacq2016building}.

However, to keep the child engaged, the robot must learn at the right rate, not too fast otherwise the kid will have
no opportunity for improving his skills and not too slow otherwise he may lose
trust in his ability to improve the robot' skills. And a learning curve just based on the middle point between last state and demonstration will be too fast for one child, and too slow for another one. We need to make the robot aware of the child's perception of it progresses to take decision about its own learning curve. This awareness should be taken in account in our mutual modelling implementation, described in section~\ref{arch}. 

\subsection{Long-term studies}\label{long}
We tested CoWriter in real pedagogic/therapeutic contexts with
children in difficulty over repeated long sessions (40-60 min). Through 3 different
case studies, we explored and refined experimental designs and algorithms in
order for the robot to adapt to the
troubles of each child and to promote their motivation and self-confidence. We report positive observations, suggesting commitment of children to help the
robot, and their comprehension that they were good enough to be teachers,
overcoming their initial low confidence with handwriting. We detail the measures and results of these studies in the following 3 subsections:
\paragraph{Case-study 1: Vincent}
\subparagraph{Context}
Vincent\footnote{The names of children has been changed.} is a five year-old child. At school, he has difficulties to learn writing, particularly with cursive letters. From our perspective, Vincent is shy and quiet. He suffers from poor self-confidence much more than any actual writing problem. The experiment was conducted without any therapist, in our laboratory. A parent was here to accompany the child, but she did not intervene during interactions. Children's personalities, conditions and state evaluation were reported by the parent.
\subparagraph{Hypothesis}
The CoWriter activity needs a child engaged as interaction leader. 
With this study we consider the problem of long-term interactions. We hypothesize that with an appealing scenario children can maintain motivation in doing a handwriting activity for an hour over 4 sessions.
\subparagraph{Methodology}
Our goal was to provide Vincent with
an environment that would enable him to sustain engagement over four one-hour sessions, 
one session per week. We decided to introduce a scenario to elicit a strong ``prot\'eg\'e effect" and such induce a stronger commitment. While the child came with low motivation in writing exercise for himself, our idea was to use this effect to promote a new extrinsic motivation: improving letters in order to help the robot. Details of the experimental design are presented in~\cite{jacq2016building}.
\subparagraph{Measures \& Analysis}
We measured the commitment of the child with the number of demonstration he provided. We also measured the duration of sessions. During the two last sessions, we recorded the time taken by the child to write each demonstration. After the experiment we interviewed the parent of the child. She was asked if she observed any impact of our activity on the child.
We compared the number of demonstrations provided by Vincent along the 4 sessions (reported on Table~\ref{table:vincent_sess}) and we summed the time spend by the child to write demonstration during the 2 last sessions.
\begin{table}[!]
    \centering
    \caption{\footnotesize Number of demonstrations provided by Vincent over the four sessions.}
    \begin{tabular}{lccccc}
        \toprule
        Session & S1 & S2 & S3 & S4 & Total\\ 
        Number of demonstrations & 23 & 34 & 52 & 46 & 155\\ 
        \bottomrule

    \end{tabular}
    \label{table:vincent_sess}
\end{table}
\subparagraph{results}
Overall, Vincent provided 155 demonstrations to the robot. We can see in Table~\ref{table:vincent_sess} that the number of demonstrations provided by Vincent was globally increasing along sessions while the difficulty of the activity was also increasing. Interestingly, as the number of demonstration decreased from session 3 to session 4, the total time spend to write demonstrations is similar: 41.6s in session 3 ($\thicksim$0.8s per letter) and 41.1s in session 4 ($\thicksim$0.89s per letter). A explanation of this result could be that since the difficulty was increasing the child spent more time to write his demonstrations.

After the first week, he showed
confidence when playing with his ``prot\'eg\'e" and he built affective bonds with the robot
over the course of the study, as evidenced by some cries on the last session,
and several letters sent to the robot \emph{after} the end of the study
(one of them 4 months later) to get news. This represents a promising initial
result: we can effectively keep a child committed into the activity with the robot for a relatively
long periods of time (about 4 hours).

From the parent's perspective, Vincent was actually showing a new motivation in improving his handwriting. He took pleasure to work with the robot and to accomplish his teacher's mission. She confirmed that an affection of the child for the robot took root within the experiment. Finally she saw an improvement of his handwriting and explained that the child ``passed from a mix of script and cursive writing up to a full-cursive writing".

But no conclusion can be drawn in terms of actual handwriting remediation: we did
not design this study to formally assess possible improvements. However, as pictured on Figure~\ref{fig:stimuli}, Vincent was able to
significantly improve the robot's skill, and he acknowledged that he had been
able to help the robot: in that regard, Vincent convinced himself that he was
``good enough'' at writing to help someone else, which is likely to have
a positive impact on his self-esteem.

\begin{figure}
    \centering
    \subfigure[Initial letter, generated by the robot]{
        \includegraphics[height=4.5cm]{diego-initial-letter}
    }
    \subfigure[Final letter, after training with Vincent]{
        \includegraphics[height=4.5cm]{diego-final-letter}
    }

    \caption{\small (French) text generated by the robot, before and after a one
        hour long interaction session with the child. As an example, the red box
        highlights the changes on the word ``envoyer''.}

    \label{fig:stimuli}
\end{figure}

\paragraph{Case-study 2: Thomas}
\subparagraph{Context}
Thomas, 5.5 years old child, is under the care of an occupational
therapist. He has been diagnosed with visuo-constructive deficits.
He was frequently performing random attempts and then was comparing
with the provided template. According to the therapist, Thomas is restless and careless: he
rarely pays attention to
advice and does not take care of his drawing movement when he is writing. He is
quickly shifting his attention from one activity to another. Thomas was working on number allographs with his therapist. During a prior
meeting, the therapist provided us with a sequence of numbers
written by Thomas. one of the observed problems was drawing
horizontally-inverted allographs, mainly for ``5". The experiment was conduced with Thomas' therapist. 
\subparagraph{Hypothesis}
We want to see if the CoWriter activity can be adapted to a pedagogical context in order help a child with diagnosed deficits to learn handwriting. We believe that small modifications of the activity adapted to
Thomas problems (visuo-constructive deficits and inattention) could help to
keep him focused on the activity during forty-minutes sessions, and to evidence to the child that the robot is progressing by dint of his demonstrations. 
\subparagraph{Methodology}
The experiment was conducted in the therapist's office (four sessions 
spanning over 5 weeks). We assumed that a scenario like the one we used 
for Vincent would not be usable with Thomas. We just introduced the robot 
and quickly said that it was seeking help to train for a robot handwriting contest. In order to integrate our work with that of the therapist, we decided to adapt the 
CoWriter activity to work with numbers. Details of the experimental design are presented in~\cite{jacq2016building}.
\subparagraph{Measures \& Analysis}
We recorded all the demonstration performed by the child and by the robot. The duration of sessions and the time spend by demonstration were also recorded by the logs of the tablet. It was difficult to make comparison between different sessions since the child did not work on the same numbers. But we could study the evolution of the quality of Thomas' demonstration when he was working on a given number (Figure~\ref{Thomas_progress}).
To show how Thomas leaded the robot to reach his level we plotted on the same graph the evolution of the quality of Thomas' demonstrations and the robot's trials (Figure~\ref{Thomas_distances}). We also reconstructed and displayed the drawn allographs of the number 6 to visualize the impact of the lessons of Thomas on the robot (Figure~\ref{learning_6_demos}). 
\begin{figure}[!]
    \centering
    \includegraphics[width=0.6\linewidth]{learning_6_demos}
    \caption{\small Demonstrations provided by Thomas for the number ``6'' (top row) and
        corresponding shapes generated by the robot. After eight demonstrations,
        Thomas decided that the robot's ``6'' was good enough, and went to
    another character: in that respect, he was the one leading the learning
process of the robot.}
    \label{learning_6_demos}
\end{figure}
\subparagraph{Results}
Despite his attention deficit, Thomas was able to remain engaged in the activity during more than
forty minutes in each session. In total, 55 allographs out of 82 
demonstrated by the child were acceptable considering our threshold (with a
progressive improvement from 13 out of 28 in the first session up to 26 out
of 29 in the last session).

As soon as Thomas understood that the robot was only accepting well-formed
allographs, he started to focus on it and he would typically draw 5 or 6 times
the number before actually sending to the robot (the tablet lets children
clear their drawing and try again before sending it). According to
the therapist, it was the first time that Thomas corrected himself in such a
way: he mad the effort to take into account how \emph{another agent} (the robot) would
interpret and understand his writing. Figure~\ref{Thomas_progress} shows how
he gradually improved his demonstrations for some numbers, according to the
metric we used to make the robot accept/refuse samples.

Since the robot's handwriting started from a simple primitive (a stroke), each
time Thomas succeeded to have his demonstrations accepted by it, the robot's
improvement was clearly visible (as measured in Figure~\ref{Thomas_distances}).
This led to a self-rewarding situation that effectively supported Thomas'
commitment.

\begin{figure}[!]
    \centering
    \includegraphics[width=0.6\linewidth]{learning_6_distances}
    \caption{\small Distance between demonstrations and templates. Green lines represent the robot performance,
blue lines performance of the child. The round IDs correspond to the demonstrations
pictured on Figure~\ref{learning_6_demos}.}
    \label{Thomas_distances}
\end{figure}

\begin{figure}[!]
    \centering
    \subfigure[number 2]{
        \includegraphics[width=6cm]{henry2}
    }
	\subfigure[number 5]{
        \includegraphics[width=6cm]{henry5}
    }
    \caption{\small Improvement of Thomas demonstrations for some numbers: a) the number 2 and b) the number 5. Thomas progressively took care of the demonstrations he was providing to the robot for those numbers. We used for this figure the same metric than the one used for the acceptance algorithm to measure distance between demonstration and templates. Distances are normalized with respect to the biggest value. The dashed line correspond to the threshold of robot's acceptance.}
    \label{Thomas_progress}
\end{figure}
\paragraph{Clinic-study: when children evaluate the robot}
\subparagraph{Context}
Each of previous studies was specifically adapted to a particular child: we relied on two different
designs in order to sustain each child's commitment.
In this new experiment, we conducted a study with eight children using a single experimental design. The children all have in common difficulties to learn
cursive writing but the nature and magnitude of these troubles are significantly
different from one child to another. Valerie (7 years old), Antoine (6.5) and
Johan (7) are under the care of an
occupational therapist. Emilien (8) and Mathieu (7) are repeating their school year
because of writing. Marie (6) and Adele (8) are bottom of their respective
classes in writing activities. Nicolas (7) is under the care of a neurologist, and
has been diagnosed with specific language impairment. Given their school year, all of these children would be
expected to know the shape of
cursive letters. The experiment was conducted in collaboration with an occupational therapist. Our goal was to study the perception of the robot's progress in children. We wanted to know how easily children were able to take the role of teachers and to detect improvements or eventual degradations of the robot's letters.
\subparagraph{Hypothesis}
Children understand their role and find motivation to teach the robot. They are able to perceive the progress of the robot, and their evaluations correlates with its handwriting performance.
\subparagraph{Methodology}
This experiment took place in an occupational therapist clinic
in Normandy, France. Over a period of two weeks, each child came three times for a one hour long
session (except Adele and Marie who only attended one session). An experimenter was present to explain the rules of the game and tablet usage. As in the previous experiments, children were provided with two tablets: one to choose a word (or a single letter) to teach, one
used by both the child and the robot to write. We also provided printed templates for the letters if the child asked for them. 
Besides, we added two buttons to the tablet interface: a green one with a ``thumbs up", and
a red one with a ``thumbs down". Those buttons could be used by the children to evaluate the
robot (the green one was for positive feedback while the red one was for negative feedback). 
We used it as a measure of the perception of the robot by the child: the more the
child used evaluation buttons, the more he was adopting the role of the teacher, judging the
robot instead of himself. Children were free to use the buttons whenever they wanted during the experiment.
\subparagraph{Measures \& Analysis}
As in previous studies, we recorded the timestamps of all demonstrations, the duration of demonstrations and we measured the overall commitment of the children as the number of demonstrations provided per session. 
We also logged all the evaluations provided by the children. The awareness of children for the robot progress is measured as the correlation between children evaluations and distances between the robot's letters and reference templates.
Since sessions took place over only two weeks, we did not attempt to study possible
handwriting remediation in children, and we focused instead on the correlation between the children's evaluations and the robot's progression.
We estimated the robot's progression as the difference between an initial score
(score of the first robot's attempt when the children have chosen a new word/letter to
work on) and the current robot's score (after being taught by the child). The
score is calculated as the average of the euclidean
distance between the robot's generated letter and the reference allograph for each of the letters of the
word. The reference letters where manually created beforehand, based on typical cursive letters template\footnote{\url{http://www.education.com/slideshow/cursive-handwriting-z/}}. At every turn, we associate two values: the current score of the robot, and the child's immediate feedback (+1 if the child pressed the green button, -1 if he pressed the red one, 0 if he did not press any button during the round). We only keep rounds with feedback (\ie a non-zero grade) and computed a Pearson's correlation between the robot score and the child feedback.
\subparagraph{Results}
All children maintained their engagement during all the sessions. They provided
on average 42 demonstrations per session. All children made use of the evaluation buttons and
had preference to reward the robot (in total, 99 positive feedbacks and 33 negative ones were recorded). Interestingly, the time spent by the children to draw the demonstrations systematically increased from one session to the other. We interpret this result as the children being more careful and demonstrating the correct gestures to the robot in a slower fashion.

We found that five children out of the eight provided evaluations that significantly correlated with progress of the robot. The coefficients of correlation $r_{robot}$ are reported in Table~\ref{table:scores}.

We also computed the correlation between the children's evaluations and their own
progress. The analysis was conducted in the same way, using distances between the children's demonstrations and reference allographs as a progress score.
The evaluations of three out of the five children whose evaluations correlated with the robot's progress, were also significantly correlated with their own progress ($r_{child}$ in Table~\ref{table:scores}). For
those children, it seems that the robot was reflecting their own performances, and while they
were judging the robot positively (three times more positive feedback than negative feedback),
they were actually evaluating themselves.


\begin{table}
    \centering
    \caption{\footnotesize Feedback from the children to the robot. \emph{\#Demo}
        denotes the average number of demonstrations per session provided by the child;
        \emph{\#Pos} and \emph{\#Neg} the total number of positive (resp.
        negative) feedbacks they provided. $r$ (robot) is the correlation coefficient
        between the feedback provided by the children and the performance of the
        robot. $r$ (child) is the correlation coefficient
        between the feedback provided by the children and their own progress.
        }
    \begin{tabular}{ccccll}
        \toprule
        \bf Child      & \bf \# Demo & \bf \# Pos & \bf \# Neg & $r_{robot}$ & $r_{child}$ \\ \midrule
        \emph{Val\'erie} & 42           & 24              & 6               & 0.25 \small\tt ** & 0.14 \small\it ns\\ 
        \emph{\'Emilien} & 74           & 20              & 9               & 0.06 \small\it ns & 0.02 \small\it ns\\
        \emph{Mathieu} & 43           & 10              & 3               & 0.23 \small\tt ** & 0.21 \small\tt **\\
        \emph{Nicolas} & 38           & 16              & 4               & 0.31 \small\tt *** & 0.20 \small\tt **\\
        \emph{Johan}   & 32           & 10              & 5               & 0.10 \small\it ns & 0.03 \small\it ns\\
        \emph{Antoine} & 27           & 10              & 3               & 0.20 \small\tt * & -0.02 \small\it ns \\
        \emph{Ad\`ele}   & 35           & 4               & 2               & 0.28 \small\tt * & 0.30 \small\tt ** \\
        \emph{Marie}   & 40           & 5               & 1               & -0.02 \small\it ns & 0.13 \small\it ns\\ \bottomrule
    \end{tabular}

    \label{table:scores}
\end{table}


\subsection{With-me-ness}\label{with}
\emph{With-me-ness}, a concept borrowed from the
field of {\it Computer-Supported Collaborative Learning}, measures in a
well-defined way to what extent the human is \emph{with} the robot over the
course of an interactive task. As such, it is a meaningful precursor of
engagement. 
\paragraph{Description}
We explored a methodology, from real-time
estimation of the human's focus of attention (relying on a novel, open-source,
vision-based head pose estimator), to on-line computation of with-me-ness. The process can be described in 3 main steps (details in~\cite{lemaignan2016realtime}):

\subparagraph{Head-pose estimation}
We derive the visual field of attention from the head pose. Our
technique only involves a single monocular RGB camera used for facial feature
extraction, and a static simplified 3D mesh of a human head.  68 facial features
are extracted using a fast template-based face alignment algorithm by Kazemi and
Sullivan~\cite{kazemi2014one}, as implemented in the open-source {\tt dlib}
library~\cite{dlib09}.  Eight of these features (chosen to be far apart and
relatively stable across age and gender) are then matched to their 3D
counterparts and we rely on an iterative $PnP$
algorithm (OpenCV's implementation) to compute the translation and rotation of
the head with respect to the camera frame. With this approach, knowing the
intrinsic parameters of the camera (calibrated camera) is required for an
accurate estimation of the absolute 3D localization of the head.

\subparagraph{Field and focus of attention estimation}
We model the field of attention as the central region of the field of view.  The
field of view itself is approximated to a cone spanned from the nasal depression
(sellion) of the human face. Different dimensions for the human field of view
can be found in the literature: Holmqvist~\cite{holmqvist2011eye} models it
with an horizontal aperture of $ \pm 40\degree $ and a vertical aperture of $
\pm 25\degree $, while Walker~\cite{walker1980clinical} for instance suggests
$60\degree$ up, $75\degree$ down, $60\degree$ inwards (towards the nose) and
$95\degree$ outwards.  Previous work on visual perspective taking for social
robotics~\cite{sisbot2011situation} model the field of attention as a cone of
$30\degree$. We retained in this work a slightly wider aperture of $40\degree$.
We then approximate the visual \emph{focus} of attention (VFoA) of the human to
the objects which lie inside this field of attention (Figure~\ref{fig:vfoa}). At
a given time, more than one object can therefore be \emph{in focus}.

\subparagraph{Computing with-me-ness}
The concept of \emph{with-me-ness} has been introduced in the field of
\emph{Computer Supported Collaborative Learning} (CSCL) by Sharma~\etal
in~\cite{sharma2014me}. Sharma~\etal introduce this concept in an attempt to
answer a recurrent teacher's question: {\it ``how much are the students with
me?''}. They distinguish what they call \emph{perceptual with-me-ness} (the
student follows what the teacher refers to with deictic gestures) from
\emph{conceptual with-me-ness} (the student follows what the teacher refers to
verbally), and they show in an eye-tracking study that \emph{conceptual
with-me-ness} in particular correlates with better learning performance. This
also relates to the concept of gaze cross-recurrence that has been shown to
reflect the quality of the interaction~\cite{jermann2012effects} in
collaborative learning tasks. Sharma~\etal simply define \emph{conceptual with-me-ness} as the normalized
percentage of time during which the student's gaze overlapped the areas of
teaching slides currently referred to by the teacher.  In order to apply it to
human-robot interactions, we propose to extend this concept, and to define
\emph{conceptual with-me-ness} as the normalized ratio of time that the human
interactant focuses its attention on the attentional target expected by the
robot for the current task (or sub-task).
\paragraph{Validation}
We validated this approach with an experiment involving the CoWriter activity in a school.
The robot
controller would associate a set of expected attentional targets to the phase of
the interaction. For instance, while the robot was
waiting for the child's handwriting demonstration (``\textit{Waiting for
feedback}'' phase), the expected attentional target of the child was the tablet (since
the child was supposed to write there) or the secondary tablet (that displayed a
template of the word, used as a reference by the child). These expected targets
(green lines on Figure~\ref{fig:with-me-ness}) form the robot's attentional {\it
a priori} knowledge and are used to compute the with-me-ness. The with-me-ness plotted at the bottom of Figure~\ref{fig:with-me-ness} is computed on a sliding window of 30 seconds, and thus gives a picture of
``how well the child is following the robot's expectations'' at that time. As
seen, the with-me-ness computed at run-time by the robot (blue line) is
generally lower than the ground-truth (orange line, based on video-annotations),
and sometimes quite off, such as during episode marked ``A'': during that phase,
one can notice that the attention is mostly directed to undefined target {\sf
Other}, likely a consequence of inaccurate head detection.  

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{with-me-ness}
    \caption{\small \textbf{With-me-ness}. Evolution of the level of
        \emph{with-me-ness} over the whole $\approx$17min long interaction of
        the child. The bottom
        diagram represents the instantaneous level of with-me-ness over a
        sliding window of 30 seconds. The blue line is the with-me-ness as estimated
        by the robot, the orange line is the with-me-ness computed from manually
        annotated attentional targets.  Pearson's correlation between both
        series for this subject: $r(973)=0.58, p < .001$.}

    \label{fig:with-me-ness}
\end{figure*}

\subsection{Importance of these results in future works}
CoWriter is an rich interaction, mostly non-verbal, full of misunderstandings between the child and the robot. For many reasons:
\begin{itemize}

\item The learning curve of the robot is several time not adapted to what expects the child (the robot is to slow to learn a word while the child is providing high number of perfect demonstrations or conversely too fast)
\item Most of the time the robot does not look what the child want him to look or, contrariwise, the child is not looking what it is expected to look (that can be detected by the with-me-ness module presented above). 
\item When the child does a mistake (pushing a wrong button on the tablet or writing wrong letters as a demonstration) the robot could detect it and react in consequence.
\item Sometimes the child start to be completely disengages and the robot could also react (by trying to call back the commitment of the child or by asking to stop the activity by itself).
\item The robot should wait to have the attention of the child in order to make sure his trial of writing will be observed.
\item The robot should react as a student, but as a \textit{pretending} student in a didactic activity: if the child provides good feedback but is teaching a very wrong writing to the robot, the robot should be able to detect this situation and to say he does not want to learn this style. 

\end{itemize}
All those situation requires a second level of mutual modelling to be solved by an unique reasoning.
The goal of my thesis is to build a cognitive architecture based on reasoning in two orders of mutual modelling. This architecture is expected to be generalizable and usable in different activities. But in order to make sure that this ability bring an improvement to HRI we need an interaction to test it. This interaction must be studied over long-term sessions in order to facilitate the grounding of non-verbal mutual understandings, and to promote occurrences of misunderstanding situations. 
With the first case-study, we proved that the CoWriter activity is sustainable by one child over (at least) four sessions of one hour. The second case-study was to show that the activity can be used in real therapeutic context and could be an help for therapists: by improving this activity, a mutual modelling architecture could have a direct utility both in education and occupational therapy. The buttons for feedback tested by the clinic-study and the VFoA tracker tested to compute with-me-ness will be a useful feature for mutual modelling. In the clinic-study, we viewed that children can provide the robot with coherent feedback which is a strong information about their perception of a robot as a student while they are the teachers. The VFoA tracker will be used to keep a robust knowledge of what the child has seen and is looking at. This knowledge is essential to reason with 1st and 2nd level of mutual modelling. Furthermore, we believe that the interaction could be strongly improved by adding some micro-behaviour of the robot (short gazes or arm's gestures, non-verbal language) that such abilities.

\section{Framework for mutual modelling in robotics}\label{framework}

\subsection{Justification of the approach}

A first intuition for mutual modelling (MM) is to assume that all agents have the same architecture. In~\cite{breazeal2006using}, Breazeal shows a MM-based reasoning where the robot uses its own architecture to model other agents. We can imagine a second level of modelling where the robot recursively attribute to other agents the mutual modelling ability. But that can create an infinite recursive loop: the agent then models the robot that models the agent etc. Another reasons to avoid an infinite recursive approach is that different agents must have different behaviours: in similar situations, they do not necessary take similar decisions. 

We propose a different approach of modelling, where we define two orders of agent's model: the \textbf{first-order-agents} concern direct representations of agents by the robot (for example the child), while the \textbf{second-order-agents} concern the representation of agents by agents (for example the robot-perceived-by-the-child). 
Modelling second-order-agent like the robot-perceived-by-the-child, helps to model how the child perceives the robot, e.g. to make sure the child understand that the robot is learning from his demonstrations. 
We can as well define $\textbf{n}^{\textbf{th}}$\textbf{-order-agents} with a higher level of theory of mind. But taking into account high levels of mutual modelling would be difficult to process in real time. Unlike the epistemic logic, our proposed framework will not take into account infinite regress~\cite{clark1991grounding} of mutual modelling.

\subsection{Notations}
$M_R\left[\textit{A}\right]$ stands for \textcolor{wine}{the model (built by the robot R) about the agent A} (first-order-agent). $M_R\left[\textit{A,B}\right]$ stands for \textcolor{wine}{the model (built by the robot R) about the agent B perceived by the agent A} (second-order-agent). 
As said above, we limit our approach to 1st and 2nd order of modelling. In a two-agents interaction (the child and the robot) we will focus on three models: $ M_R\left[\textit{C}\right]$ (the model about the child), $ M_R\left[\textit{R}\right]$ (the model about the robot) and $ M_R\left[\textit{C,R}\right]$ (the robot perceived by the child). 
It would be also interesting to study $ M_R\left[\textit{C,C}\right]$, the model about the child perceived by himself in order to play with his self-confidence. But detecting differences between $ M_R\left[\textit{C}\right]$ and $ M_R\left[\textit{C,C}\right]$ seems difficult with the current abilities of the robot. 

As models are dynamic, $ M^t_R\left[\textit{A}\right]$ represent the model about an agent A at time $t$.

\subsection{Mutual understanding}
Given those three models ($ M_R\left[\textit{R}\right]$, $ M_R\left[\textit{C}\right]$ and $ M_R\left[\textit{C,R}\right]$) the robot must be able to detect misunderstandings. 
A misunderstanding of an agent $A$ by the robot can be formalised as a error between what is actually in the mind of the agent (we can call it $\Phi[A]$) and the model built by the robot: $\Delta \left(\Phi[A] ; M_R\left[\textit{A}\right]\right)$. But if $A$ is human, $\Phi[A]$ is inaccessible to the robot. In order to maintain a mutual understanding, humans~\cite{suzuki2015neural} (and monkeys~\cite{haroush2015neuronal}), use predictions of others' behaviours. A bio-inspired approach would be to make, at time $t$, a prediction $P^{t+1}_R\left[\textit{A}\right]$ of the model. Then, at time $t+1$, the robot can compute a \textbf{prediction error} $\Delta \left( M^{t+1}_R\left[\textit{A}\right]; P^{t+1}_R\left[\textit{A}\right]\right)$ in order to detect such a misunderstanding. This idea rely on the assumption that the better are the predictions of a model, the better the model fits the reality. Then, the dynamic of the model can be updated according to the resulting error of prediction. This rule can be used with $ M_R\left[\textit{C}\right]$ and $ M_R\left[\textit{C,R}\right]$. 

Another type of misunderstanding concerns the comprehension of the robot by the child: using the same formalism, it is an error between the actual perception of the robot by the child (we can call it $\Phi[C,R]$) and the robot itself: $\Delta \left(\Phi[C,R] ; M_R\left[\textit{R}\right]\right)$. Again, the robot does not have access to $\Phi[C,R]$, but it approximates it with $ M_R\left[\textit{C,R}\right]$. Finally we define the \textbf{child's perception error} at time $t$ by $\Delta \left(M^t_R\left[\textit{C,R}\right] ; M^t_R\left[\textit{R}\right]\right)$. This error is taken in account only if the robot has a correct model of itself perceived by the child (only if $M_R\left[\textit{C,R}\right]$ produce small prediction errors). Since this error assumes that models built by the robot are correct, it is not used to update these models. It corresponds to an error of the child: in order to repair it, the robot must explain the misunderstanding to the child or exaggerate an action to make sure it will be understood.

As an example, in the CoWriter activity, the child teach handwriting to the robot. The robot pretends to be a beginner, but it has its idea of a good handwriting. The robot is perfectly aware of its played progresses. But we want the child to be aware of the progress of the robot. In that perspective, the child's perception error corresponds to the sentence ``\textit{I make progress but the child think I don't}", while the prediction error of $M_R\left[\textit{C,R}\right]$ corresponds to ``\textit{The child think I don't make progress, but maybe he think I do and I haven't understood him}".  

\subsection{Structure and dynamic of the models}
Each model contains the knowledge and possible actions of an agent. The dynamic between actions and knowledges must be encoded. Some causality can be assumed in advance (if the child gives a good feedback to the robot, it means that the robot-perceived-by-the-child makes progresses) while other can be learnt (e.g. if each time the robot looks at the head of the child the child stops to write). Each modelled agent has its own goals, encoded as a reinforcement learning process. As an example, in CoWriter the robot-perceived-by-the-child ($ M_R\left[\textit{C,R}\right]$) is a learner and has the simple goal to make progress. The child ($ M_R\left[\textit{C}\right]$) is modelled as a teacher and is assumed to have the goal to improve the robot's writing. Then, the goal of the robot ($ M_R\left[\textit{R}\right]$) is to keep a mutual understanding: its knowledges and actions must correspond to itself perceived by the child, and it must be able to predict the behaviour of the child. The schema on Figure~\ref{mm} visually summarizes this example.

The goals of $ M_R\left[\textit{C}\right]$ and $ M_R\left[\textit{C,R}\right]$ depend on the activity and must be programmed each time we move to a new activity. They can be improved to better fit the child's behaviour by learning from experience. Contrariwise, the goal of $ M_R\left[\textit{R}\right]$ to keep a mutual understanding is independent and reusable. 

\begin{figure}[!]
\centering
\includegraphics[width=0.6\columnwidth]{mutual_behaviour}
\caption{\small\textbf{}  }
\label{mm}
\end{figure} 

\section{Cognitive architecture}\label{arch}

\subsection{Global description}

Our cognitive architecture for mutual modelling can be designed in three main parts. A \textbf{perception part} will regroup all the modules that measure the values of the perceived variables using sensitive devices (camera, sensors, micros...). An example of module of the sensitive part could be the system developed by Lemaignan~\cite{lemaignan2016realtime} that uses camera to measure a value of \textit{with-me-ness}. These values are sent to the \textbf{mutual modelling part} that updates in real-time models with measured values of perceived variables and deducts the value of abstract variables. Finally, the \textbf{decision part} contains all the modules associated to the control of the robot (and other active devices like tablets in CoWriter). These modules can read values given by mutual models in order to compute decisions. In the example of the CoWriter activity, these modules are given by the system that learns and generate letters, but we can add a module that generates micro-behaviour, another that decides to switch to a new activity (e.g. drawing with the robot),... etc. The following subsections explain in detail the content and operation of the three parts of the architecture. 

\subsection{Perception modules}

The sensitive modules contain all the module able to use sensitive devices to measure value of relevant perceived variables. Some of these modules are associated with the content of the interaction activity. In CoWriter, the learning module takes inputs of tablets to compute the new state of robot's writing. It defines a sensitive module, and the value of the new state of the robot to write a letter defines a perceived variable. As well, the evaluation of the robot by the child via the feedback buttons defines another perceived variable provided by the modules of the activity. Other modules are independent of the activity: the system that estimates the target objects looked by the child provides additive information not directly used by the modules of the activity.  

\subsection{Mutual modelling modules}

Each perceived value measured by sensitive modules are associated with the model of an agent (or a $n^{th}$-order agent). Each mutual model can be designed as a module that knows the list of its associated perceived variables and watch if the value of one of these variables has been changed. An additive module knows all the expected causalities and computes the values of abstract variables. Some expected causalities can be empirically learned and other set by hands. 

\subsection{Decision making}

We believe that the values of variables provided by mutual modelling will provide rich and useful information for decision making. Taking in account these values to elaborate decision should improve the realism and the efficiency of the robot in the interaction.
Just like the sensitive ones, the modules that take decision can be directly associated to the activity (in CoWriter, the choice of a new learning curve or the decision to suddenly make a big mistake), or can govern independent behaviour (for example a module that generate the micro-behaviour of the robot).

Some decisions, especially the one that will govern the micro-behaviours of the robot can be autonomously made by the robot. It will not strongly affect the content and objectives of the activity (in CoWriter the main objective is to provide the child with a new extrinsic motivation to write in helping the robot). But other decisions can have a high impact on the progress of the interaction: to stop an activity and to switch to a new one can frustrate a child that was committed into the activity. The conditions to make such a decision are not directly assessable, but must be learned by the robot. In order to make these decision cautiously, we propose to start by a Wizard-of-Oz approach and to move towards an autonomous approach following these steps: 
\begin{enumerate}
\item \textbf{Wizard-of-Oz}: A human takes decisions; the robot learns
\item \textbf{Mixed-initiative}: The robot makes suggestions; a human agrees or disagrees
\item \textbf{Autonomous}: The robot makes decisions
\end{enumerate}

The picture~\ref{cog} visually summarize the global design of our architecture. 

\begin{figure}[!]
\centering
\includegraphics[width=1\columnwidth]{cognitive_archi}
\caption{\small\textbf{Overview of the cognitive architecture}. Yellow squares represent different main parts. White ellipses represent modules. We named in example some content of modules and illustrated possible devices used for perception and decisions. }
\label{cog}
\end{figure}

\section{Implementation}

\subsection{ROS modules}

The architecture will be implemented using ROS to synchronize the activity of different modules (written as ROS nodes). The perceived variables provided by sensitive modules and abstract variables provided by expected causalities will be published as messages on topics specific to the associated models. In the same way, modules of decision part will subscribe to the model's topic in order to watch the values of the variables that they need. 

\subsection{Integration to CoWriter}

The CoWriter activity is already implemented using ROS. The main nodes that measure values of relevant variables are the robot's state machine, the node that governs the algorithm to learn/generate letters and the interpreter of the tablet inputs. Variables are already published on other topics used by the activity. We will additionally publish them on the topics associated to the mutual models. 

We will use four models: the model of the robot $\textbf{M}_\textbf{R}$, the model of the child $\textbf{M}_\textbf{C}$, the model of the robot-perceived-by-the-child $\textbf{M}_\textbf{C,R}$ and the model of the child-perceived-by-the-child $\textbf{M}_\textbf{C,C}$.

Currently, the decisions made within the CoWriter activity that needs information from mutual models are the physical behaviour of the robot (for example, when the robot is waiting for a demonstration it looks the tablet) and the progress of the robot (the choice of the new state learned from the demonstration). We can add several decisions in order to improve the realism and the smoothness of the interaction (for example, the robot suddenly makes an exaggerated mistake, the robot changes its learning curve ...). If we want to take into account information from mutual modelling, we need to improve the algorithms of decision since we bring additive variables: for example, to change the learning curve will require some values of variables associated with $\textbf{M}_\textbf{C,R}$ like the feedback of the child from tablet's buttons.

\section{Evaluation}

\subsection{Hypothesis}

The question of this thesis concerns the improvement of human-robot interactions brought by mutual modelling, especially in the educational context of the CoWriter activity. In order to evaluate this improvement, we hypothesize the following assertion:\\
\textit{Decisions made using additive information from second level of mutual modelling improve the quality of the CoWriter interaction.}\\
To study this hypothesis, we need to rigorously define the \textit{quality of the interaction}. Since the goal is to promote child's self-esteem and extrinsic motivation by inducing a ``prot\'eg\'e" effect, we would like to measure the motivation of the child to play the activity and his satisfaction of the robot's progress.

\subsection{Experimental studies}

Such a measure can be defined by a large amount of variable that provide cues about the commitment of the child (quantity of demonstrations, time spend to write demonstrations, quality/progress of demonstration, progress of the robot, \textit{with-me-ness} ...). The evaluation with feedback buttons can also be used to estimate how the child understands his role of teacher~\cite{jacq2016building}. 

Along the successive steps to develop our mutual modelling architecture (each step being the addition of a new variable or a group of new variables to take into account), we will test the improvement of the interaction by measuring the above values. We will promote long-term studies with children facing real difficulties to learn handwriting with professional facilitators (teachers or therapists), following our previous works~\cite{jacq2016building}. 

\section{Conclusion and planning}

We presented in this paper the research question of this thesis, which concerns the importance of mutual modelling in educative human-robot interactions. In order to study this question, we will develop an architecture based on mutual modelling that will collect information about mental and physical states of different agent. The stages of this construction will correspond to the successive additions of variables (or groups of variables) to take into account by the models. We will progressively extract pieces of response to our research question by testing our system each time a stage is crossed. We will use the framework for mutual modelling in HRI that we introduced in~\ref{framework} in order to simplify notations and clarify the concepts. 

During the first year of this thesis, our work was turned on the improvement and finishing of the CoWriter activity's implementation. We designed an experimental set-up that enable long-term studies in pedagogic/therapeutic contexts with children facing real difficulties to learn handwriting~\cite{jacq2016building}. Finally, we started to implement additional sensitive modules independent of the CoWriter that will provide relevant variables to feed our future mutual modelling architecture (e.g. the system that estimates what the visual object of attention of the child~\cite{lemaignan2016realtime}).


\bibliographystyle{abbrv}
\small
\bibliography{biblio} 

\end{document}
